<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Benchmarks | SMT-COMP</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Benchmarks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The International Satisfiability Modulo Theories (SMT) Competition." />
<meta property="og:description" content="The International Satisfiability Modulo Theories (SMT) Competition." />
<link rel="canonical" href="https://smt-comp.github.io/2014/benchmarks.html" />
<meta property="og:url" content="https://smt-comp.github.io/2014/benchmarks.html" />
<meta property="og:site_name" content="SMT-COMP" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Benchmarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"The International Satisfiability Modulo Theories (SMT) Competition.","headline":"Benchmarks","url":"https://smt-comp.github.io/2014/benchmarks.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css?v=ca87ca498acb8424db393bc72e2497c3e747f945">
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <style> a { color: #D86850; } </style>
    
  </head>
  <body>
    <div class="wrapper">
        <header>
  <h1>
    <a href="/">SMT-COMP</a>
  </h1>

  

  <p>The International Satisfiability Modulo Theories (SMT) Competition.</p>

  <p class="view">
    <a class="hl" href="https://github.com/smt-comp"><b>GitHub</b></a>
  </p>
  <p class="view">
    <a href="/index.html"><b>Home</b></a><br/>
    <a href="/introduction.html"><b>Introduction</b></a><br/>
    <a href="/benchmark_submission.html"><b>Benchmark Submission</b></a><br/>
    <a href="/publications.html"><b>Publications</b></a><br/>
    <a class="hl" href="https://smtlib.cs.uiowa.edu/"><b>SMT-LIB</b></a></br>
    <a class="hl" href="/previous.html"><b>Previous Editions</b></a></br>
  </p>
  <p class="view">
    <h3><a href="/2014/">SMT-COMP 2014</a>
    </h3>
    <a href="/2014/rules14.pdf"><b>Rules</b></a><br/><a href="/2014/benchmarks.html"><b>Benchmarks</b></a><br/><a href="/2014/tools.html"><b>Tools</b></a><br/><a href="/2014/specs.html"><b>Specs</b></a><br/><a href="/2014/participants.html"><b>Participants</b></a><br/><a href="/2014/results.html"><b>Results</b></a><br/><a href="https://doi.org/10.3233/sat190109"><b>Report</b></a><br/>
  </p>

  
</header>


      <section>

<h2 id="benchmarks">Benchmarks</h2>

<p>The SMT Competition will use a subset of the benchmarks available within SMT-LIB, as described
in the competition rules. New benchmarks are especially encouraged by the May 15 deadline. The set of benchmarks will be announced as soon as is feasible after that deadline. Please adhere to the following instructions in submitting new benchmarks:</p>

<ul>
  <li>Benchmarks should be organized into separate directories by logic.</li>
  <li>Within each logic, there should be one or more directories containing
the benchmarks - you can choose the names of the directories, but they
should be different from any names currently existing in SMT-LIB for
that logic.</li>
  <li>Each benchmark should contain the following metadata:</li>
</ul>
<pre>
(set-logic &lt;logic&gt;)
(set-info :source |&lt;information about where the benchmark came from
including author contact, paper citations, etc.&gt;|)
(set-info :smt-lib-version 2.0)
(set-info :category &lt;either "industrial", "crafted", or "random"&gt;) 
(set-info :status &lt;either sat, unsat, or unknown&gt;)
</pre>
<p>In :category, the quotes are needed in the benchmark file.</p>

<p>Thanks to the following for new benchmarks in 2014:</p>
<ul>
  <li>Clark Barrett</li>
  <li>Philipp Wendler -rec’d</li>
  <li>Aaron Tomb et al. (Galois) -rec’d</li>
  <li>Gergely Kovasznai</li>
  <li>Matthias Heizmann</li>
  <li>Hristina Palikareva</li>
  <li>…</li>
</ul>

<h3 id="benchmark-set-and-difficulty-measurements">Benchmark Set And Difficulty Measurements</h3>

<p>The benchmarks used in the competition are selected from the 
SMT-LIB benchmark set. The complete set is reduced by a few restrictions:</p>
<ul>
  <li>The correct result (sat or unsat) must be known</li>
  <li>Trivial benchmarks defined in the rules as those that are quickly solved by all solvers in the previous competition or evaluation.</li>
  <li>Any random selection is biased toward benchmarks categorized as “industrial” and toward more difficulty benchmarks.</li>
</ul>

<p>For the last point a measurement of difficulty is needed. This is an approximate measure meant simply to make the competition more challenging. For 2014 we used the value of the fastest solver on that benchmark in the 2013 evaluation. For benchmarks introduced since then, an approximate value was determined by taking the best value from running one or more solvers from the evaluation during the
preparation for the 2014 competition.</p>

<p>The table from which benchmarks will be chosen for the competition is available <a href="difficulties-2014.txt">here</a>. 
The table will be updated during the course of preparing for the competition as benchmarks are corrected, removed because they are ill-formed or inappropriate, moved among logics as needed, or difficuly or expected results determined.</p>

<p>The table is in space-separated form that is very amenable to text processing tools such as awk, grep and sort. You can turn it into a comma-separated table for Excel or similar tools by using sed or tr to turn spaces into commas.</p>

<p>The columns in the table are these:</p>

<ol>
  <li><strong>id</strong> - the StarExec benchmark id. Note that this id will change if the benchmark is corrected. This is happening frequently during the preparation for the competition. The ids will be resynchronized with StarExec just prior to the final selection.</li>
  <li><strong>difficulty</strong> - this value is in seconds, or is one of the words “trivial” or “unknown”. “trivial” benchmarks are those that have been determmined to be easy for every solver and so excluded by the rules. “unknown” means the difficulty value has not yet been determined. Valaues of 1500 or 1600 generally indicate that all solvers quit without a result (timeout or memout).</li>
  <li><strong>starexec-expected-result</strong> - this is the metadata value from StarExec and is the correct value expected for the benchmark. This should be one of 
“sat”, “unsat” or “unknown” (without the quotes). However, ther eare also values of “null” (metadata value is not set).</li>
  <li><strong>status</strong> - this is the :status value from the benchmark, as recorded in the StarExec metadata. It should be precisely the same as the starexec-expected-result.</li>
  <li><strong>category</strong> - this is the :category value as recorded in StarExec metadata, which should have determined it from a set-info command in the benchmark. This value should be one of “industrial”, “crafted”, “random”, or “check” (without the quotes). Values of “null” (category not set), “unknown” (category explicitly set to the value “unknown”), and “none” (value in StarExec is an empty string) are also present.</li>
  <li><strong>logic</strong> - the SMT-LIB logic</li>
  <li><strong>path</strong> - the path of the benchmark within the non-incremental subspace of benchmarks.</li>
</ol>

      </section>
      <footer>
        <!--

<p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
-->

      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>

