<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>SMT-COMP 2019 call for comments, benchmarks, solvers | SMT-COMP</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="SMT-COMP 2019 call for comments, benchmarks, solvers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="14th International Satisfiability Modulo Theories Competition (SMT-COMP’19)" />
<meta property="og:description" content="14th International Satisfiability Modulo Theories Competition (SMT-COMP’19)" />
<link rel="canonical" href="https://smt-comp.github.io/2019/news/2019-01-24" />
<meta property="og:url" content="https://smt-comp.github.io/2019/news/2019-01-24" />
<meta property="og:site_name" content="SMT-COMP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-29T18:22:10+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="SMT-COMP 2019 call for comments, benchmarks, solvers" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-01-29T18:22:10+00:00","datePublished":"2024-01-29T18:22:10+00:00","description":"14th International Satisfiability Modulo Theories Competition (SMT-COMP’19)","headline":"SMT-COMP 2019 call for comments, benchmarks, solvers","mainEntityOfPage":{"@type":"WebPage","@id":"https://smt-comp.github.io/2019/news/2019-01-24"},"url":"https://smt-comp.github.io/2019/news/2019-01-24"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css?v=ca87ca498acb8424db393bc72e2497c3e747f945">
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <style> a { color: #c30000; } </style>
  </head>
  <body>
    <div class="wrapper">
        <header>
  <h1>
    <a href="/">SMT-COMP</a>
  </h1>

  

  <p>The International Satisfiability Modulo Theories (SMT) Competition.</p>

  <p class="view">
    <a class="hl" href="https://github.com/smt-comp"><b>GitHub</b></a>
  </p>
  <p class="view">
    <a href="/index.html"><b>Home</b></a><br/>
    <a href="/introduction.html"><b>Introduction</b></a><br/>
    <a href="/benchmark_submission.html"><b>Benchmark Submission</b></a><br/>
    <a href="/publications.html"><b>Publications</b></a><br/>
    <a class="hl" href="https://smtlib.cs.uiowa.edu/"><b>SMT-LIB</b></a></br>
    <a class="hl" href="/previous.html"><b>Previous Editions</b></a></br>
  </p>
  <p class="view">
    <h3><a href="/2019/">SMT-COMP 2019</a>
    </h3>
    <a href="/2019/rules19.pdf"><b>Rules</b></a><br/><a href="/2019/benchmarks.html"><b>Benchmarks</b></a><br/><a href="/2019/tools.html"><b>Tools</b></a><br/><a href="/2019/specs.html"><b>Specs</b></a><br/><a href="/2019/participants.html"><b>Participants</b></a><br/><a href="/2019/results.html"><b>Results</b></a><br/><a href="/2019/SMT-COMP-2019-slides.pdf"><b>Slides</b></a><br/>
  </p>

  
</header>


      <section>

<h2>SMT-COMP 2019 call for comments, benchmarks, solvers </h2>
<p>22 Jan 2019

<h3 id="14th-international-satisfiability-modulo-theories-competition-smt-comp19">14th International Satisfiability Modulo Theories Competition (SMT-COMP’19)</h3>

<p>July 7-8, 2019<br />
Lisbon, Portugal</p>

<p>CALL FOR COMMENTS<br />
CALL FOR BENCHMARKS<br />
PRELIMINARY CALL FOR SOLVERS</p>

<hr />

<p>We are pleased to announce the 2019 edition of SMT-COMP.</p>

<p>SMT-COMP is the annual competition among Satisfiability Modulo
Theories (SMT) solvers.</p>

<p>The goals of SMT-COMP’19 are to encourage scientific advances in the
power and scope of solvers, to stimulate the community to explore and
discuss shared challenges, to promote tools and their usage, to engage
and include new members of the community (in a fun environment) and to
support the SMT-LIB project in its efforts to promote and develop the
SMT-LIB format and collect and collate relevant benchmarks.</p>

<p>The results of SMT-COMP’19 will be announced at the SMT Workshop (July
7-8), which is affiliated with the 22nd International Conference on
Theory and Applications of Satisfiability Testing (SAT 2019).</p>

<p>SMT-COMP’19 is organized under the direction of the SMT Steering
committee.<br />
The organizing team for SMT-COMP’19 is:</p>

<ul>
  <li>Liana Hadarean - Amazon, USA</li>
  <li><a href="https://www.inf.usi.ch/postdoc/hyvarinen/">Antti Hyvarinen</a> - Universita della Svizzera italiana, CH</li>
  <li><a href="https://cs.stanford.edu/people/niemetz">Aina Niemetz</a> (co-chair) - Stanford University, USA</li>
  <li><a href="http://www.cs.man.ac.uk/~regerg/">Giles Reger</a> (co-chair) - University of Manchester, UK</li>
</ul>

<p>This is a call for three things:</p>

<h4 id="call-for-comments">CALL FOR COMMENTS:</h4>

<p>The organizing team is preparing the schedule and rules for 2019. To
further the above goals, we propose to make several changes to the
format of SMT-COMP’19.</p>

<p>Any comments you may have on these proposed changes, on how to improve
the competition or to redirect its focus are welcome and will be
considered by the team.<br />
We particularly appreciate comments received
before <em>February 11th, 2019</em>.</p>

<ol>
  <li>
    <p><strong>Benchmark selection</strong></p>

    <p>Since 2015, the organizers of SMT-COMP chose to evaluate all solvers
 on all relevant benchmarks. As a consequence, results became more
 predictable, and the fixed set of benchmarks may encourage solver
 developers to overfit the problem set. Overall, this seems more of an
 evaluation than a competition.</p>

    <p>Furthermore many of the problems in SMT-LIB can now be considered
 ‘easy’, or at least ‘unsurprising’. For example, in the last iteration
 of the competition 78% of the 258,741 main track benchmarks were
 solved by all supported solvers within the time limit (71% within 1
 second). In 7 (out of 46) logics, over 99% of benchmarks were solved
 by all solvers.</p>

    <p>To shift the focus towards challenging benchmarks, reduce the number
 of benchmarks to be able to run with a longer timeout, and bring back
 the competitive aspect, we propose the following alternative benchmark
 selection scheme:</p>

    <ul>
      <li>
        <p>As a first step we will remove all benchmarks in a division that
were solved by <em>all</em> solvers (including non-competing solvers) in
this division in under a second during last year’s competition.</p>
      </li>
      <li>
        <p>From the remaining benchmarks we will randomly select a minimum of
300 benchmarks. If the division contains less, we include all
benchmarks.  If 50% of the eligible benchmarks in the division
exceed 300, we select 50% of the benchmarks.</p>
      </li>
      <li>
        <p>Benchmarks will be (pseudo)-randomly selected without considering
the difficulty, category, or family of a benchmark.  Rationale:
difficulty is hard to quantify and easy benchmarks are already
removed, categories are already reasonably distributed (around 78%
of the non-incremental and almost 100% of the incremental
benchmarks are labelled ‘industrial’) and benchmark weights
normalized with respect to family size are allowed for in the
scoring formula.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Increase time limit to 40 minutes</strong></p>

    <p>In 2017, the competition time limit was decreased to 20 minutes due to
 the infeasibility of running all submitted solvers on all benchmarks
 of a division. The new benchmark selection strategy enables us to
 extend the time limit back to 40 minutes.</p>
  </li>
  <li>
    <p><strong>Different application domains require different time limits</strong></p>

    <p>For example software verification traditionally requires much lower
 time limits, compared to hardware verification.
 To reward solvers optimized for different use cases, we propose two
 new tracks this year:</p>

    <ol>
      <li>
        <p><em>New industry challenge track</em></p>

        <p>This track will contain unsolved SMT-LIB benchmarks, with an
emphasis on those coming from industrial applications. We will also
include benchmarks nominated by the community as challenging and of
interest. In this track, solvers will run with a significantly
longer time limit, e.g., several hours. We additionally encourage
the community to submit new benchmarks for this track.</p>
      </li>
      <li>
        <p><em>New 24-second score</em></p>

        <p>In the main track the competition currently gives separate scores
for sequential and parallel performance. To reward tools that solve
problems quickly we will introduce a third ‘24-second score’ for
the number of problems solved within 24 seconds (wall clock time).</p>
      </li>
    </ol>
  </li>
  <li>
    <p><strong>Mandatory system descriptions for submitted solvers</strong></p>

    <p>As part of a submission, SMT-COMP entrants are now required to provide
 a short (1–2 pages) description of the system. This should include a
 list of all authors of the system, their present institutional
 affiliations, and any appropriate acknowledgements. The programming
 language(s) and basic SMT solving approach employed should be
 described (e.g., lazy integration of a Nelson-Oppen combination with
 SAT, translation to SAT, etc.). System descriptions should also
 include a URL for a web site for the submitted tool.</p>

    <p>The main incentive for this change is twofold. First, we want to
 improve transparency when submitted solvers are wrapper tools
 according to the rules of the competition.  Second, we want to
 encourage documentation of technical improvements that lead to the
 current results.</p>
  </li>
  <li>
    <p><strong>Do not run non-competitive divisions</strong></p>

    <p>A division in a track is competitive if at least two substantially
 different solvers (i.e., solvers from two different teams) were
 submitted. Although the organizers may enter other solvers for
 comparison purposes, only solvers that are explicitly submitted by
 their authors determine whether a division is competitive, and are
 eligible to be designated as winners.</p>
  </li>
  <li>
    <p><strong>New experimental model generation track for QF_BV</strong></p>

    <p>In many SMT applications, model generation is an essential feature.
 Currently, none of the SMT-COMP tracks require model generation. One
 of the challenges is that the model format is not consistent across
 different solvers. While imposing a standard over all logics is
 challenging, there are several logics where it is straightforward. For
 this reason, this year we are planning to include a new experimental
 model generation track for QF_BV. In the future we hope to expand this
 track to other logics as a way of pushing for model standardization.</p>
  </li>
  <li>
    <p><strong>Rename tracks</strong></p>

    <p>We will rename ‘Application Track’ to ‘Incremental Track’ and ‘Main
 Track’ to ‘Single Problem Track’.</p>

    <p>We believe the current names are misleading, as the current ‘Main
 Track’ also contains problems coming from applications.  Additionally,
 having it called ‘Main’ de-emphasizes the importance of the other
 tracks and use cases of SMT.</p>
  </li>
</ol>

<p><strong>We would additionally like to pose the following open ended questions
to the community:</strong></p>

<ol>
  <li>
    <p><em>Should SMT-COMP include new logics?</em></p>

    <ul>
      <li>Last year strings were an experimental division (and the semantics
are still not fixed). Should this remain experimental? How should
we encourage new solvers and benchmarks for this track?</li>
      <li>A new SMT-LIB theory for reals with transcendental functions is
also in progress. Is it mature enough to feature in the
competition (as an experimental division)?</li>
    </ul>
  </li>
  <li>
    <p><em>How can we fix the current scoring scheme?</em></p>

    <p>In 2016 a new scoring scheme was introduced, where benchmark
 weights are normalized with respect to their family size. The goal
 was to not have large benchmark families dominate the solver score
 (as opposed to simply counting the number of correctly solved
  instances while adding a penalty for incorrect results). The
 current scoring system, however, now emphasizes very small
 benchmark families (now these families are what may dominate the
 score). We would appreciate suggestions for a new scoring scheme
 that strikes a balance between these two trends.</p>
  </li>
  <li>
    <p><em>What is a benchmark family?</em></p>

    <p>The previous question assumes benchmarks are sensibly collected
 into families. The previous rules use the directory structure to
 define families with each family given by a top-level directory.
 However, it is not clear that the way that SMT-LIB is curated
 leads to sensible families with this approach as the current
 approach is to place benchmarks submitted by one person for a
 logic into a single directory – there is no reason to assume that
 these benchmarks are related. We suggest that using the lowest
 level directory structure may be more appropriate but are there
 any alternative definitions of family that could be used?</p>
  </li>
  <li>
    <p><em>Is it important to have ‘overall’ winners?</em></p>

    <p>If so, how should we achieve this? The current approach is
 completely dependent on the number of logics supported by the
 tool. If not overall winners, do we want a higher-level notion of
 winner than a division winner?  What is the best way to
 acknowledge the success of solvers and motivate newcomers whilst
 keeping the competition fun?</p>
  </li>
</ol>

<h4 id="call-for-benchmarks">CALL FOR BENCHMARKS:</h4>

<p>Have interesting or hard benchmarks that can be made public? Want the
world’s best SMT solvers to compete to solve <em>your</em> problems? Submit
your benchmarks to SMT-LIB and SMT-COMP!</p>

<p>Please let us know as soon as possible if you are considering
submitting benchmarks, even if the material is not quite ready. We
will work in close cooperation with the SMT-LIB maintainers to
integrate such benchmarks into SMT-LIB. The deadline for submission of
new benchmarks to be used in the 2019 competition is
<em>March 1st, 2019</em>.</p>

<p>We would encourage new benchmarks in the following logics (which
appear to have ‘stagnated’ in the sense that the benchmarks in
them are no longer challenging to competitive solvers):</p>

<ul>
  <li>ALIA</li>
  <li>AUFLIA</li>
  <li>AUFNIRA</li>
  <li>NIA</li>
  <li>NRA</li>
  <li>QF_ANIA</li>
  <li>QF_AUFBV</li>
  <li>QF_AUFNIA</li>
  <li>QF_DT</li>
  <li>QF_FP</li>
  <li>QF_LIRA</li>
  <li>QF_NIRA</li>
  <li>QF_RDL</li>
  <li>QF_UFBV</li>
  <li>QF_UFIDL</li>
  <li>QF_UFLIA</li>
  <li>QF_UFLRA</li>
  <li>QF_UFNIA</li>
  <li>QF_UFNRA</li>
  <li>UFBV</li>
  <li>UFIDL</li>
  <li>UFLRA</li>
</ul>

<p>We would also like to extend our call for benchmarks to <em>include</em>
benchmarks with some <strong>additional information</strong>:</p>

<ol>
  <li>
    <p>For the Industrial Challenge Track we would like to receive
difficult benchmarks that are important to you and either unsolved,
or unsolved within some reasonable time limit. We would
particularly like benchmarks that come with a description of why
they are difficult/important. Of course, if this is not possible
then new challenging benchmarks are always appreciated.</p>
  </li>
  <li>
    <p>We would appreciate receiving benchmarks that you want solved
quickly (e.g.  in under 24 seconds) but currently struggle to.
Please add the required solution time as a comment to the
benchmark. If we receive many benchmarks of this kind we may
consider a new track in the future that specifically focuses on
benchmarks requiring short time limits.</p>
  </li>
</ol>

<h4 id="preliminary-call-for-solvers">PRELIMINARY CALL FOR SOLVERS:</h4>

<p>A submission deadline for solvers will be announced along with the
rules.  However, it is useful to the organizing team to know in
advance which and how many solvers may be entering. If you have not
submitted a solver before, or if you think there may be unusual
circumstances, we request that you let us know at your earliest
convenience if you think you may be submitting one or more solvers to
SMT-COMP’19. Note that this year we require a system description for
all submitted solvers as part of the submission of the final solver
versions.</p>

<h4 id="communication">COMMUNICATION:</h4>

<p>The competition website will be at <a href="http://www.smtcomp.og">www.smtcomp.org</a>.</p>

<p>The SMT-COMP repository will move to GitHub at <a href="https://github.com/smt-comp">https://github.com/smt-comp</a>.</p>

<p>Public email regarding the competition may be sent to smt-comp@cs.nyu.edu.</p>

<p>Announcements will be sent to both smt-comp@cs.nyu.edu and smt-announce@googlegroups.com.</p>

<p>Sincerely,</p>

<p>The organizing team</p>



</p>

      </section>
      <footer>
        <!--

<p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
-->

      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>

