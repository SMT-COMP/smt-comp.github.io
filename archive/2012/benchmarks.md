## Benchmarks

<b> Note: The benchmark archives that were included on this page are lost and
not available anymore. The following information is kept mainly for archival
purposes.</b>

These benchmarks and tools are subject to change until their respective freeze
dates, as documented in the <a href="rules.html">SMT-COMP rules</a>.


### Application Track Benchmarks

#### 2012 Additions
- <b>safari</b> - derived from SMT-queries of an infinite-state model-checking tool (from R. Bruttomesso, SAFARI tool, CAV 2012).
- <b>smtic3_QF_LRA</b> - benchmarks derived from an IC3 algorithm applied to some software verification benchmarks (from A. Griggio).

#### Benchmarks from 2011
- <b>blast_simplify_calls</b> - *QF_UFLIA benchmarks, from Blast.*<br/>
 These are just logs of the calls to the Simplify theorem prover, used
 by Blast when trying to model check some C programs (the name of the
 program is reflected in the name of the benchmark -- original Simplify traces
 can be made available for those who are interested).

- <b>hybrid_networks</b> - *QF_LRA bounded model-checking (BMC) and k-Induction problems on networks of hybrid automata, from NuSMV.*<br/>
 These are benchmarks for hybrid automata, generated by unrolling the NuSMV models and checking
 them with BMC or k-Induction

 - <b>kratos_systemC_swmc</b> - *QF_BV bounded model-checking (BMC) and k-Induction problems on SystemC designs, from NuSMV.*<br/>
 Unrollings of translation of some SystemC programs into NuSMV. The programs were those used, e.g., in the FMCAD'10 paper:
 <i>Verifying SystemC: a Software Model Checking Approach</i> by Alessandro Cimatti, Andrea Micheli, Iman Narasamdya and Marco Roveri
 The two sets are essentially the same, except for the different logic
 used.

- <b>kratos_systemC_swmc</b> - *QF_LIA bounded model-checking (BMC) and k-Induction problems on SystemC designs, from NuSMV.*<br/>
 Unrollings of translation of some SystemC programs into
 NuSMV. The programs were those used, e.g., in the FMCAD'10 paper:
 <i>Verifying SystemC: a Software Model Checking Approach</i> by
 Alessandro Cimatti, Andrea Micheli, Iman Narasamdya and Marco Roveri
 The two sets are essentially the same, except for the different logic
 used.

- <b>lustre</b> - *QF_LIA BMC and k-Induction problems on Lustre programs, from NuSMV.*<br/>
 These were obtained from subset of the Lustre models also used for
 the KIND set, except that they were generated from a NuSMV version of
 the Lustre programs, by NuSMV itself.

- <b>kind</b> - *QF_UFLIA traces from KIND, postprocessed (for competition.*<br/>
 These benchmarks were obtained from the KIND tool during Lustre programs verification.

- <b>asasp_20110606</b> - *ASASP benchmarks.*<br/>
 [ASASP](https://st.fbk.eu/technologies/asasp) implements a symbolic reachability
procedure for the analysis of administrative access control policies. A more
detailed description of the benchmarks can be found in the following paper:
 <a href="https://dl.acm.org/doi/10.1145/1966913.1966935">Efficient Symbolic Automated Analysis of Administrative Attribute-based RBAC-Policies,</a> by F.  Alberti, A. Armando, and S. Ranise.

### Unsat Core Track Benchmarks
The unsat core benchmarks are the subset of the benchmarks from the indicated 
logics that are unsatisfiable; the benchmarks themselves have been modified
to include names for the assertions. Per the SMTLIB standard, the benchmarks
are allowed to contain a mix of named and unmaned formulae, though ordinarily,
all top-level formulae will have names. The names may be scrambled by the
benchmark scrambler.

The competition benchmarks were selected from these divisions:
- QF_LIA (2584 benchmarks)
- QF_LRA (317 benchmarks)
- QF_IDL (683 benchmarks)
- QF_BV (1399 benchmarks)

### Format for Results Files

<b>Note: The Results archives that were provided with every benchmarks archive
are lost and not available anymore. The following information is
obsolete and only kept for archival purposes.</b>

<p>
Each tarball contains one or two status files for each
benchmark:
</p>

<ul>
  <li><code>BENCHMARK.results.txt</code> is always present, and</li>
  <li><code>BENCHMARK.results_untrusted.txt</code> might be present as well</li>
</ul>

<p>
These files contain one status per line, corresponding to the series of
<code>(check-sat)</code> commands in the benchmarks.
</p>

<p>
Each status line (where different from &quot;unknown&quot;) has been determined by
running at least 3 different SMT solvers on the set of instances
resulted from &quot;unfolding&quot; each incremental benchmark using the
scrambler. For <code>*.results.txt</code>, all the results different from &quot;unknown&quot;
have been reported by at least 2 different solvers, whereas all the
status lines generated by a single solver only (because e.g. the others
timed out) have been replaced with &quot;unknown&quot;. For
<code>*.results_untrusted.txt</code>, the single-solver
answer is also included. However, they are marked with an &quot;# untrusted&quot; comment.
</p>


<!--#include virtual="smt-comp-postlude.shtml" -->
