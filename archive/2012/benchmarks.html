<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Benchmarks | SMT-COMP</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Benchmarks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The International Satisfiability Modulo Theories (SMT) Competition." />
<meta property="og:description" content="The International Satisfiability Modulo Theories (SMT) Competition." />
<link rel="canonical" href="https://smt-comp.github.io/2012/benchmarks.html" />
<meta property="og:url" content="https://smt-comp.github.io/2012/benchmarks.html" />
<meta property="og:site_name" content="SMT-COMP" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Benchmarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"The International Satisfiability Modulo Theories (SMT) Competition.","headline":"Benchmarks","url":"https://smt-comp.github.io/2012/benchmarks.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css?v=ca87ca498acb8424db393bc72e2497c3e747f945">
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <style> a { color: #D9A360; } </style>
    
  </head>
  <body>
    <div class="wrapper">
        <header>
  <h1>
    <a href="/">SMT-COMP</a>
  </h1>

  

  <p>The International Satisfiability Modulo Theories (SMT) Competition.</p>

  <p class="view">
    <a class="hl" href="https://github.com/smt-comp"><b>GitHub</b></a>
  </p>
  <p class="view">
    <a href="/index.html"><b>Home</b></a><br/>
    <a href="/introduction.html"><b>Introduction</b></a><br/>
    <a href="/benchmark_submission.html"><b>Benchmark Submission</b></a><br/>
    <a href="/publications.html"><b>Publications</b></a><br/>
    <a class="hl" href="https://smtlib.cs.uiowa.edu/"><b>SMT-LIB</b></a></br>
    <a class="hl" href="/previous.html"><b>Previous Editions</b></a></br>
  </p>
  <p class="view">
    <h3><a href="/2012/">SMT-COMP 2012</a>
    </h3>
    <a href="/2012/rules.html"><b>Rules</b></a><br/><a href="/2012/benchmarks.html"><b>Benchmarks</b></a><br/><a href="/2012/tools.html"><b>Tools</b></a><br/><a href="/2012/specs.html"><b>Specs</b></a><br/><a href="/2012/participants.html"><b>Participants</b></a><br/><a href="/2012/results.html"><b>Results</b></a><br/><a href="https://doi.org/10.29007/gj66"><b>Report</b></a><br/>
  </p>

  
</header>


      <section>

<h2 id="benchmarks">Benchmarks</h2>

<p><b> Note: The benchmark archives that were included on this page are lost and
not available anymore. The following information is kept mainly for archival
purposes.</b></p>

<p>These benchmarks and tools are subject to change until their respective freeze
dates, as documented in the <a href="rules.html">SMT-COMP rules</a>.</p>

<h3 id="application-track-benchmarks">Application Track Benchmarks</h3>

<h4 id="2012-additions">2012 Additions</h4>
<ul>
  <li><b>safari</b> - derived from SMT-queries of an infinite-state model-checking tool (from R. Bruttomesso, SAFARI tool, CAV 2012).</li>
  <li><b>smtic3_QF_LRA</b> - benchmarks derived from an IC3 algorithm applied to some software verification benchmarks (from A. Griggio).</li>
</ul>

<h4 id="benchmarks-from-2011">Benchmarks from 2011</h4>
<ul>
  <li>
    <p><b>blast_simplify_calls</b> - <em>QF_UFLIA benchmarks, from Blast.</em><br />
 These are just logs of the calls to the Simplify theorem prover, used
 by Blast when trying to model check some C programs (the name of the
 program is reflected in the name of the benchmark – original Simplify traces
 can be made available for those who are interested).</p>
  </li>
  <li>
    <p><b>hybrid_networks</b> - <em>QF_LRA bounded model-checking (BMC) and k-Induction problems on networks of hybrid automata, from NuSMV.</em><br />
 These are benchmarks for hybrid automata, generated by unrolling the NuSMV models and checking
 them with BMC or k-Induction</p>
  </li>
  <li>
    <p><b>kratos_systemC_swmc</b> - <em>QF_BV bounded model-checking (BMC) and k-Induction problems on SystemC designs, from NuSMV.</em><br />
 Unrollings of translation of some SystemC programs into NuSMV. The programs were those used, e.g., in the FMCAD’10 paper:
 <i>Verifying SystemC: a Software Model Checking Approach</i> by Alessandro Cimatti, Andrea Micheli, Iman Narasamdya and Marco Roveri
 The two sets are essentially the same, except for the different logic
 used.</p>
  </li>
  <li>
    <p><b>kratos_systemC_swmc</b> - <em>QF_LIA bounded model-checking (BMC) and k-Induction problems on SystemC designs, from NuSMV.</em><br />
 Unrollings of translation of some SystemC programs into
 NuSMV. The programs were those used, e.g., in the FMCAD’10 paper:
 <i>Verifying SystemC: a Software Model Checking Approach</i> by
 Alessandro Cimatti, Andrea Micheli, Iman Narasamdya and Marco Roveri
 The two sets are essentially the same, except for the different logic
 used.</p>
  </li>
  <li>
    <p><b>lustre</b> - <em>QF_LIA BMC and k-Induction problems on Lustre programs, from NuSMV.</em><br />
 These were obtained from subset of the Lustre models also used for
 the KIND set, except that they were generated from a NuSMV version of
 the Lustre programs, by NuSMV itself.</p>
  </li>
  <li>
    <p><b>kind</b> - <em>QF_UFLIA traces from KIND, postprocessed (for competition.</em><br />
 These benchmarks were obtained from the KIND tool during Lustre programs verification.</p>
  </li>
  <li>
    <p><b>asasp_20110606</b> - <em>ASASP benchmarks.</em><br />
 <a href="https://st.fbk.eu/technologies/asasp">ASASP</a> implements a symbolic reachability
procedure for the analysis of administrative access control policies. A more
detailed description of the benchmarks can be found in the following paper:
 <a href="https://dl.acm.org/doi/10.1145/1966913.1966935">Efficient Symbolic Automated Analysis of Administrative Attribute-based RBAC-Policies,</a> by F.  Alberti, A. Armando, and S. Ranise.</p>
  </li>
</ul>

<h3 id="unsat-core-track-benchmarks">Unsat Core Track Benchmarks</h3>
<p>The unsat core benchmarks are the subset of the benchmarks from the indicated 
logics that are unsatisfiable; the benchmarks themselves have been modified
to include names for the assertions. Per the SMTLIB standard, the benchmarks
are allowed to contain a mix of named and unmaned formulae, though ordinarily,
all top-level formulae will have names. The names may be scrambled by the
benchmark scrambler.</p>

<p>The competition benchmarks were selected from these divisions:</p>
<ul>
  <li>QF_LIA (2584 benchmarks)</li>
  <li>QF_LRA (317 benchmarks)</li>
  <li>QF_IDL (683 benchmarks)</li>
  <li>QF_BV (1399 benchmarks)</li>
</ul>

<h3 id="format-for-results-files">Format for Results Files</h3>

<p><b>Note: The Results archives that were provided with every benchmarks archive
are lost and not available anymore. The following information is
obsolete and only kept for archival purposes.</b></p>

<p>
Each tarball contains one or two status files for each
benchmark:
</p>

<ul>
  <li><code>BENCHMARK.results.txt</code> is always present, and</li>
  <li><code>BENCHMARK.results_untrusted.txt</code> might be present as well</li>
</ul>

<p>
These files contain one status per line, corresponding to the series of
<code>(check-sat)</code> commands in the benchmarks.
</p>

<p>
Each status line (where different from &quot;unknown&quot;) has been determined by
running at least 3 different SMT solvers on the set of instances
resulted from &quot;unfolding&quot; each incremental benchmark using the
scrambler. For <code>*.results.txt</code>, all the results different from &quot;unknown&quot;
have been reported by at least 2 different solvers, whereas all the
status lines generated by a single solver only (because e.g. the others
timed out) have been replaced with &quot;unknown&quot;. For
<code>*.results_untrusted.txt</code>, the single-solver
answer is also included. However, they are marked with an &quot;# untrusted&quot; comment.
</p>

<!--#include virtual="smt-comp-postlude.shtml" -->

      </section>
      <footer>
        <!--

<p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
-->

      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>

