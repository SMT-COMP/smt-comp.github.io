<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Benchmarks | SMT-COMP</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Benchmarks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The International Satisfiability Modulo Theories (SMT) Competition." />
<meta property="og:description" content="The International Satisfiability Modulo Theories (SMT) Competition." />
<link rel="canonical" href="https://smt-comp.github.io/2011/benchmarks.html" />
<meta property="og:url" content="https://smt-comp.github.io/2011/benchmarks.html" />
<meta property="og:site_name" content="SMT-COMP" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Benchmarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"The International Satisfiability Modulo Theories (SMT) Competition.","headline":"Benchmarks","url":"https://smt-comp.github.io/2011/benchmarks.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css?v=ca87ca498acb8424db393bc72e2497c3e747f945">
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <style> a { color: #D9A360; } </style>

  </head>
  <body>
    <div class="wrapper">
        <header>
  <h1>
    <a href="/">SMT-COMP</a>
  </h1>



  <p>The International Satisfiability Modulo Theories (SMT) Competition.</p>

  <p class="view">
    <a class="hl" href="https://github.com/smt-comp"><b>GitHub</b></a>
  </p>
  <p class="view">
    <a href="/index.html"><b>Home</b></a><br/>
    <a href="/introduction.html"><b>Introduction</b></a><br/>
    <a href="/benchmark_submission.html"><b>Benchmark Submission</b></a><br/>
    <a href="/publications.html"><b>Publications</b></a><br/>
    <a class="hl" href="https://smtlib.cs.uiowa.edu/"><b>SMT-LIB</b></a></br>
    <a class="hl" href="/previous.html"><b>Previous Editions</b></a></br>
  </p>
  <p class="view">
    <h3><a href="/2011/">SMT-COMP 2011</a>
    </h3>
    <a href="/2011/rules.html"><b>Rules</b></a><br/><a href="/2011/benchmarks.html"><b>Benchmarks</b></a><br/><a href="/2011/tools.html"><b>Tools</b></a><br/><a href="/2011/specs.html"><b>Specs</b></a><br/><a href="/2011/participants.html"><b>Participants</b></a><br/><a href="/2011/results.html"><b>Results</b></a><br/>
  </p>


</header>


      <section>

<h2 id="benchmarks">Benchmarks</h2>

<p><b> Note: The benchmark archives that were included on this page are lost and
not available anymore. The following information is kept mainly for archival
    purposes.</b></p>


<p>
These benchmarks and tools are subject to change until their respective freeze
dates, as documented in the <a href="rules.shtml">SMT-COMP 2011 rules</a>.
</p>

<h2>Application Track Benchmarks</h2>
<ul>
  <li><b>QF_UFLIA benchmarks, from Blast (128MB)</b><br/>
 These are just logs of the calls to the Simplify theorem prover, used
 by Blast when trying to model check some C programs (the name of the
 program is reflected in the name of the benchmark -- original Simplify traces
 can be made available for those who are interested)<br/>
      <b>Results</b>
  <li><b>QF_LRA BMC and k-Induction problems on networks of hybrid automata, from NuSMV (161MB)</b><br/>
 These are benchmarks for hybrid automata, generated by unrolling the NuSMV models and checking
 them with BMC or k-Induction<br/>
      <b>Results</b></li>
  <li><b>QF_BV BMC and k-Induction problems on SystemC designs, from NuSMV (134MB)</b><br/>
 Unrollings of translation of some SystemC programs into NuSMV. The programs were those used, e.g., in the FMCAD'10 paper:
 <i>Verifying SystemC: a Software Model Checking Approach</i> by Alessandro Cimatti, Andrea Micheli, Iman Narasamdya and Marco Roveri
 The two sets are essentially the same, except for the different logic
 used.<br/>
      <b>Results</b></li>
  <li><b>QF_LIA BMC and k-Induction problems on SystemC designs, from NuSMV (346MB)</b><br/>
 Unrollings of translation of some SystemC programs into
 NuSMV. The programs were those used, e.g., in the FMCAD'10 paper:
 <i>Verifying SystemC: a Software Model Checking Approach</i> by
 Alessandro Cimatti, Andrea Micheli, Iman Narasamdya and Marco Roveri
 The two sets are essentially the same, except for the different logic
 used.<br/>
      <b>Results</b></li>
  <li><b>QF_LIA BMC and k-Induction problems on Lustre programs, from NuSMV (172MB)</b><br/>
 These were obtained from subset of the Lustre models also used for
 the KIND set, except that they were generated from a NuSMV version of
 the Lustre programs, by NuSMV itself.<br/>
      <b>Results</b></li>
  <li><b>QF_UFLIA traces from KIND, postprocessed (for competition, 128MB).</b><br/>
 These benchmarks were obtained from the KIND tool during Lustre programs verification<br/>
      <b>Results</b><br/>
      <b>Original traces are also available, they aren't in the restricted format for competition, e.g., they use define-fun.  They also are mis-labeled as QF_UFIDL. (4.2MB)</b></li>
  <li><b>ASASP benchmarks (31MB)</b><br/>
 ASASP (http://st.fbk.eu/ASASP) implements a symbolic reachability
procedure for the analysis
 of administrative access control policies. A more detailed
description of the benchmarks can be found in the following paper:<br/>
 <i>Efficient Symbolic Automated Analysis of Administrative Attribute-based RBAC-Policies,</i> by F.
Alberti, A. Armando, and S. Ranise.
 <a href="http://st.fbk.eu/sites/st.fbk.eu/files/asiaccs174-alberti.pdf">http://st.fbk.eu/sites/st.fbk.eu/files/asiaccs174-alberti.pdf</a><br/>
      <b>Results (updated 26 June 2011)</b></li>
</ul>

<h2>Format for results files</h2>

<p>
Each tarball contains one or two status files for each
benchmark:
</p>

<ul>
  <li><code>BENCHMARK.results.txt</code> is always present, and</li>
  <li><code>BENCHMARK.results_untrusted.txt</code> might be present as well</li>
</ul>

<p>
These files contain one status per line, corresponding to the series of
<code>(check-sat)</code> commands in the benchmarks.
</p>

<p>
Each status line (where different from &quot;unknown&quot;) has been determined by
running at least 3 different SMT solvers on the set of instances
resulted from &quot;unfolding&quot; each incremental benchmark using the
scrambler. For <code>*.results.txt</code>, all the results different from &quot;unknown&quot;
have been reported by at least 2 different solvers, whereas all the
status lines generated by a single solver only (because e.g. the others
timed out) have been replaced with &quot;unknown&quot;. For
<code>*.results_untrusted.txt</code>, the single-solver
answer is also included. However, they are marked with an &quot;# untrusted&quot; comment.
</p>

<h2>Tools</h2>

<p>
Tools for all competition tracks are available on the
<a href="tools.html">tools page</a>.
</p>

<!--#include virtual="smt-comp-postlude.shtml" -->

      </section>
      <footer>
        <!--

<p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
-->

      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>

  </body>
</html>
